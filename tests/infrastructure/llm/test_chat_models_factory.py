import asyncio
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from app.infrastructure.llms import llm_factory

# =============================================================================
# èŠå¤©æ¨¡å‹å·¥å‚åŠŸèƒ½éªŒè¯
# =============================================================================

async def validate_single_chat_model(provider: str, model_name: str) -> dict:
    """éªŒè¯å•ä¸ªèŠå¤©æ¨¡å‹"""
    result = {
        "provider": provider,
        "model_name": model_name,
        "model_creation": False,
        "basic_chat": False,
        "stream_chat": False,
        "tool_calling": False,
        "stream_tool_calling": False,
        "error": None
    }
    
    try:
        # 1. æ¨¡å‹åˆ›å»ºéªŒè¯
        model_supported = llm_factory.if_model_support(provider, model_name)
        if not model_supported:
            result["error"] = "æ¨¡å‹ä¸æ”¯æŒ"
            return result
        
        model_instance = llm_factory.create_model(provider=provider, model=model_name)
        if model_instance is None:
            result["error"] = "æ¨¡å‹åˆ›å»ºå¤±è´¥"
            return result
        
        result["model_creation"] = True
        print(f"   âœ… {provider}/{model_name}: æ¨¡å‹åˆ›å»ºæˆåŠŸ ({type(model_instance).__name__})")
        
        # 2. åŸºæœ¬èŠå¤©éªŒè¯
        try:
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
            user_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
            user_question = "ä½ å¥½ï¼Œè¯·ç®€å•å›ç­”ã€‚"
            
            response, token_count = await model_instance.chat(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                user_question=user_question
            )
            
            if response is not None and hasattr(response, 'content'):
                result["basic_chat"] = True
                print(f"   âœ… {provider}/{model_name}: åŸºæœ¬èŠå¤©æˆåŠŸ (Token: {token_count})")
                print(f"   ğŸ“ è¿”å›å†…å®¹: {response.content[:200]}{'...' if len(response.content) > 200 else ''}")
            else:
                result["error"] = "åŸºæœ¬èŠå¤©å¤±è´¥"
        except Exception as e:
            result["error"] = f"åŸºæœ¬èŠå¤©å¼‚å¸¸: {e}"
        
        # 3. æµå¼èŠå¤©éªŒè¯
        if result["basic_chat"]:
            try:
                system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
                user_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
                user_question = "è¯·ç”¨æµå¼æ–¹å¼å›ç­”ã€‚"
                
                stream, token_count = await model_instance.chat_stream(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    user_question=user_question
                )
                
                stream_content = ""
                chunk_count = 0
                async for chunk in stream:
                    stream_content += chunk
                    chunk_count += 1
                
                if len(stream_content) > 0:
                    result["stream_chat"] = True
                    print(f"   âœ… {provider}/{model_name}: æµå¼èŠå¤©æˆåŠŸ (Chunks: {chunk_count}, Token: {token_count})")
                    print(f"   ğŸ“ æµå¼å†…å®¹: {stream_content[:200]}{'...' if len(stream_content) > 200 else ''}")
                else:
                    result["error"] = "æµå¼èŠå¤©å¤±è´¥"
            except Exception as e:
                result["error"] = f"æµå¼èŠå¤©å¼‚å¸¸: {e}"
        
        # 4. å·¥å…·è°ƒç”¨éªŒè¯
        if result["basic_chat"]:
            try:
                tools = [
                    {
                        "type": "function",
                        "function": {
                            "name": "get_weather",
                            "description": "è·å–å¤©æ°”ä¿¡æ¯",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "city": {
                                        "type": "string",
                                        "description": "åŸå¸‚åç§°"
                                    }
                                },
                                "required": ["city"]
                            }
                        }
                    }
                ]
                
                system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·ã€‚"
                user_prompt = "è¯·ä½¿ç”¨å·¥å…·è·å–åŒ—äº¬ä»Šå¤©çš„å¤©æ°”ã€‚"
                user_question = "åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ"
                
                response, token_count = await model_instance.ask_tools(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    user_question=user_question,
                    tools=tools
                )
                
                if response is not None:
                    result["tool_calling"] = True
                    print(f"   âœ… {provider}/{model_name}: å·¥å…·è°ƒç”¨æˆåŠŸ (Token: {token_count})")
                    print(f"   ğŸ“ å·¥å…·è°ƒç”¨è¿”å›: {response.content[:200]}{'...' if len(response.content) > 200 else ''}")
                    # æ‰“å°å·¥å…·è°ƒç”¨ä¿¡æ¯
                    if hasattr(response, 'tool_calls') and response.tool_calls:
                        print(f"   ğŸ”§ å·¥å…·è°ƒç”¨ä¿¡æ¯: {response.tool_calls}")
                    elif hasattr(response, 'tool_call') and response.tool_call:
                        print(f"   ğŸ”§ å·¥å…·è°ƒç”¨ä¿¡æ¯: {response.tool_call}")
                else:
                    result["error"] = "å·¥å…·è°ƒç”¨å¤±è´¥"
            except Exception as e:
                result["error"] = f"å·¥å…·è°ƒç”¨å¼‚å¸¸: {e}"
        
        # 5. æµå¼å·¥å…·è°ƒç”¨éªŒè¯
        if result["basic_chat"]:
            try:
                tools = [
                    {
                        "type": "function",
                        "function": {
                            "name": "get_weather",
                            "description": "è·å–å¤©æ°”ä¿¡æ¯",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "city": {
                                        "type": "string",
                                        "description": "åŸå¸‚åç§°"
                                    }
                                },
                                "required": ["city"]
                            }
                        }
                    }
                ]
                
                system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·ã€‚"
                user_prompt = "è¯·ä½¿ç”¨å·¥å…·è·å–ä¸Šæµ·ä»Šå¤©çš„å¤©æ°”ã€‚"
                user_question = "ä¸Šæµ·å¤©æ°”å¦‚ä½•ï¼Ÿ"
                
                stream, token_count = await model_instance.ask_tools_stream(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    user_question=user_question,
                    tools=tools
                )
                
                stream_content = ""
                chunk_count = 0
                tool_calls_info = []
                async for chunk in stream:
                    stream_content += chunk
                    chunk_count += 1
                    # æ£€æŸ¥chunkä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ä¿¡æ¯
                    if hasattr(chunk, 'tool_calls') and chunk.tool_calls:
                        tool_calls_info.extend(chunk.tool_calls)
                    elif hasattr(chunk, 'tool_call') and chunk.tool_call:
                        tool_calls_info.append(chunk.tool_call)
                
                if len(stream_content) > 0:
                    result["stream_tool_calling"] = True
                    print(f"   âœ… {provider}/{model_name}: æµå¼å·¥å…·è°ƒç”¨æˆåŠŸ (Chunks: {chunk_count}, Token: {token_count})")
                    print(f"   ğŸ“ æµå¼å·¥å…·è°ƒç”¨å†…å®¹: {stream_content[:200]}{'...' if len(stream_content) > 200 else ''}")
                    # æ‰“å°æµå¼å·¥å…·è°ƒç”¨ä¿¡æ¯
                    if tool_calls_info:
                        print(f"   ğŸ”§ æµå¼å·¥å…·è°ƒç”¨ä¿¡æ¯: {tool_calls_info}")
                else:
                    result["error"] = "æµå¼å·¥å…·è°ƒç”¨å¤±è´¥"
            except Exception as e:
                result["error"] = f"æµå¼å·¥å…·è°ƒç”¨å¼‚å¸¸: {e}"
        
    except Exception as e:
        result["error"] = f"éªŒè¯å¼‚å¸¸: {e}"
    
    return result

async def validate_chat_models_factory():
    """èŠå¤©æ¨¡å‹å·¥å‚åŠŸèƒ½éªŒè¯"""
    print("=" * 60)
    print("ğŸ” èŠå¤©æ¨¡å‹å·¥å‚åŠŸèƒ½éªŒè¯å¼€å§‹")
    print("=" * 60)
    
    validation_results = {
        "config_loading": False,
        "supported_models": False,
        "model_creation": False,
        "error_handling": False
    }
    
    try:
        # 1. é…ç½®åŠ è½½éªŒè¯
        print("\nğŸ“‹ 1. é…ç½®åŠ è½½éªŒè¯")
        try:
            # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if hasattr(llm_factory, '_config') and llm_factory._config:
                providers = llm_factory._config.get("models", {})
                print(f"   é…ç½®åŠ è½½: âœ… é€šè¿‡ - åŠ è½½äº† {len(providers)} ä¸ªæä¾›å•†é…ç½®")
                validation_results["config_loading"] = True
            else:
                print(f"   é…ç½®åŠ è½½: âŒ å¤±è´¥ - æœªåŠ è½½åˆ°ä»»ä½•é…ç½®")
        except Exception as e:
            print(f"   é…ç½®åŠ è½½: âŒ å¤±è´¥ - {e}")
        
        # 2. æ”¯æŒçš„æ¨¡å‹éªŒè¯
        print("\nğŸ“‹ 2. æ”¯æŒçš„æ¨¡å‹éªŒè¯")
        try:
            supported_models = llm_factory.get_supported_models()
            if supported_models:
                total_models = sum(len(provider_info.get("models", {})) for provider_info in supported_models.values())
                print(f"   æ”¯æŒçš„æ¨¡å‹: âœ… é€šè¿‡ - {len(supported_models)} ä¸ªæä¾›å•†ï¼Œ{total_models} ä¸ªæ¨¡å‹")
                for provider, provider_info in supported_models.items():
                    models = provider_info.get("models", {})
                    print(f"     {provider}: {len(models)} ä¸ªæ¨¡å‹")
            else:
                print(f"   æ”¯æŒçš„æ¨¡å‹: âŒ å¤±è´¥ - æœªæ‰¾åˆ°æ”¯æŒçš„æ¨¡å‹")
            
            validation_results["supported_models"] = len(supported_models) > 0
        except Exception as e:
            print(f"   æ”¯æŒçš„æ¨¡å‹: âŒ å¤±è´¥ - {e}")
        
        # 3. æ¨¡å‹éªŒè¯
        print("\nğŸ“‹ 3. æ¨¡å‹éªŒè¯")
        try:
            # 3.1 å…ˆéªŒè¯é»˜è®¤æ¨¡å‹
            print("   3.1 é»˜è®¤æ¨¡å‹éªŒè¯")
            default_provider, default_model = llm_factory.get_default_model()
            default_result = await validate_single_chat_model(default_provider, default_model)
            
            if default_result["error"]:
                print(f"   é»˜è®¤æ¨¡å‹éªŒè¯: âŒ å¤±è´¥ - {default_result['error']}")
            else:
                print(f"   é»˜è®¤æ¨¡å‹éªŒè¯: âœ… é€šè¿‡")
                print(f"   åŸºæœ¬èŠå¤©: {'âœ…' if default_result['basic_chat'] else 'âŒ'}")
                print(f"   æµå¼èŠå¤©: {'âœ…' if default_result['stream_chat'] else 'âŒ'}")
                print(f"   å·¥å…·è°ƒç”¨: {'âœ…' if default_result['tool_calling'] else 'âŒ'}")
                print(f"   æµå¼å·¥å…·è°ƒç”¨: {'âœ…' if default_result['stream_tool_calling'] else 'âŒ'}")
            
            # 3.2 éªŒè¯æ‰€æœ‰æœ‰æ•ˆæ¨¡å‹
            print("   3.2 æ‰€æœ‰æœ‰æ•ˆæ¨¡å‹éªŒè¯")
            supported_models = llm_factory.get_supported_models()
            all_models_results = []
            
            for provider, provider_info in supported_models.items():
                models = provider_info.get("models", {})
                for model_name in models.keys():
                    print(f"\n   {'='*50}")
                    print(f"   éªŒè¯æ¨¡å‹: {provider}/{model_name}")
                    print(f"   {'='*50}")
                    result = await validate_single_chat_model(provider, model_name)
                    all_models_results.append(result)
            
            # ç»Ÿè®¡ç»“æœ
            total_models = len(all_models_results)
            successful_models = sum(1 for r in all_models_results if not r["error"])
            basic_chat_success = sum(1 for r in all_models_results if r["basic_chat"])
            stream_chat_success = sum(1 for r in all_models_results if r["stream_chat"])
            tool_calling_success = sum(1 for r in all_models_results if r["tool_calling"])
            stream_tool_calling_success = sum(1 for r in all_models_results if r["stream_tool_calling"])
            
            print(f"\n   éªŒè¯ç»“æœç»Ÿè®¡:")
            print(f"   æ€»æ¨¡å‹æ•°: {total_models}")
            print(f"   æ¨¡å‹åˆ›å»ºæˆåŠŸ: {successful_models}")
            print(f"   åŸºæœ¬èŠå¤©æˆåŠŸ: {basic_chat_success}")
            print(f"   æµå¼èŠå¤©æˆåŠŸ: {stream_chat_success}")
            print(f"   å·¥å…·è°ƒç”¨æˆåŠŸ: {tool_calling_success}")
            print(f"   æµå¼å·¥å…·è°ƒç”¨æˆåŠŸ: {stream_tool_calling_success}")
            
            validation_results["model_creation"] = successful_models > 0
        except Exception as e:
            print(f"   æ¨¡å‹éªŒè¯: âŒ å¤±è´¥ - {e}")
        
        # 4. é”™è¯¯å¤„ç†éªŒè¯
        print("\nğŸ“‹ 4. é”™è¯¯å¤„ç†éªŒè¯")
        try:
            # æµ‹è¯•æ— æ•ˆAPIå¯†é’¥ - åˆ›å»ºä¸€ä¸ªä½¿ç”¨æ— æ•ˆAPIå¯†é’¥çš„æ–°å®ä¾‹
            print("   æµ‹è¯•æ— æ•ˆAPIå¯†é’¥å¤„ç†...")
            try:
                # åˆ›å»ºä¸€ä¸ªä½¿ç”¨æ— æ•ˆAPIå¯†é’¥çš„æ¨¡å‹å®ä¾‹
                invalid_model = llm_factory.create_model(api_key="invalid_key_test_12345")
                
                response, token_count = await invalid_model.chat(
                    system_prompt="æµ‹è¯•",
                    user_prompt="æµ‹è¯•", 
                    user_question="æµ‹è¯•"
                )
                
                # æ£€æŸ¥æ˜¯å¦è¿”å›äº†é”™è¯¯å“åº”
                if response is not None and hasattr(response, 'success') and not response.success:
                    print(f"   æ— æ•ˆAPIå¯†é’¥å¤„ç†: âœ… é€šè¿‡ - æ­£ç¡®è¿”å›é”™è¯¯å“åº”")
                    validation_results["error_handling"] = True
                else:
                    print(f"   æ— æ•ˆAPIå¯†é’¥å¤„ç†: âŒ å¤±è´¥ - æœªæ­£ç¡®å¤„ç†é”™è¯¯")
                    validation_results["error_handling"] = False
            except Exception as api_error:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯ç›¸å…³çš„é”™è¯¯
                error_str = str(api_error).lower()
                if any(keyword in error_str for keyword in ['unauthorized', 'authentication', 'api key', 'invalid', '401', '403']):
                    print(f"   æ— æ•ˆAPIå¯†é’¥å¤„ç†: âœ… é€šè¿‡ - æ­£ç¡®æŠ›å‡ºè®¤è¯å¼‚å¸¸: {api_error}")
                    validation_results["error_handling"] = True
                else:
                    print(f"   æ— æ•ˆAPIå¯†é’¥å¤„ç†: âš ï¸  éƒ¨åˆ†é€šè¿‡ - æŠ›å‡ºå¼‚å¸¸ä½†éè®¤è¯é”™è¯¯: {api_error}")
                    validation_results["error_handling"] = True  # ä»ç„¶ç®—ä½œé€šè¿‡ï¼Œå› ä¸ºæ­£ç¡®å¤„ç†äº†é”™è¯¯
                    
        except Exception as e:
            print(f"   é”™è¯¯å¤„ç†: âŒ å¤±è´¥ - {e}")
            validation_results["error_handling"] = False
        
    except Exception as e:
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import logging
        logging.exception("èŠå¤©æ¨¡å‹å·¥å‚éªŒè¯å¼‚å¸¸")
    
    # è¾“å‡ºéªŒè¯ç»“æœæ±‡æ€»
    print("\n" + "=" * 60)
    print("ğŸ“Š èŠå¤©æ¨¡å‹å·¥å‚éªŒè¯ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    total_tests = len(validation_results)
    passed_tests = sum(1 for result in validation_results.values() if result)
    
    for test_name, result in validation_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
    
    print(f"\nğŸ“ˆ æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰èŠå¤©æ¨¡å‹åŠŸèƒ½éªŒè¯é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†èŠå¤©æ¨¡å‹åŠŸèƒ½éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒAPIå¯†é’¥")
    
    print("=" * 60)
    
    return validation_results

if __name__ == "__main__":
    import asyncio
    import os
    import sys
    
    # è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8
    if sys.platform == "win32":
        import codecs
        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
        sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())
    
    async def run_validation():
        """è¿è¡ŒèŠå¤©æ¨¡å‹å·¥å‚éªŒè¯"""
        try:
            # è®¾ç½®æ—¥å¿—çº§åˆ«
            import logging
            logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
            
            print("ğŸš€ å¯åŠ¨èŠå¤©æ¨¡å‹å·¥å‚åŠŸèƒ½éªŒè¯...")
            await validate_chat_models_factory()
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸  éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nğŸ’¥ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import logging
            logging.exception("èŠå¤©æ¨¡å‹å·¥å‚éªŒè¯ä¸¥é‡é”™è¯¯")
    
    # è¿è¡ŒéªŒè¯
    asyncio.run(run_validation())